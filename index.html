<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Discovering Robotic Interaction Modes with Discrete Representation Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://lichothu.github.io" target="_blank">Liquan Wang</a>,</span>
              <span class="author-block">
                <a href="https://imankgoyal.github.io" target="_blank">Ankit Goyal</a>,</span>
              <span class="author-block">
                <a href="https://jack-xhp.github.io" target="_blank">Haoping Xu</a>,</span>
              <span class="author-block">
                <a href="https://animesh.garg.tech" target="_blank">Animesh Garg</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Georgia Tech, University of Toronto, NVIDIA, Vector Institute<br>2024 Conference on Robot Learning</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                   <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2410.20258.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Supplementary PDF link -->
              <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Supplementary</span>
              </a>
            </span>

            <!-- Github link -->
            <span class="link-block">
              <a href="https://github.com/pairlab/ActAIM" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>

          <!-- ArXiv abstract Link -->
          <span class="link-block">
            <a href="https://arxiv.org/abs/2410.20258" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</div>
</div>
</div>
</section>


<!-- Teaser video-->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/corl_video.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Points</h2>
        <div class="content has-text-justified">
          <ul class="fa-ul">
            <li><span class="fa-li"><i class="fas fa-robot"></i></span>Humans use different interaction modes when manipulating objects like opening or closing a drawer.</li>
            <li><span class="fa-li"><i class="fas fa-brain"></i></span>Traditional robot learning methods lack discrete representations of these modes.</li>
            <li><span class="fa-li"><i class="fas fa-lightbulb"></i></span>We introduce ActAIM2, which learns these interaction modes without supervision.</li>
            <li><span class="fa-li"><i class="fas fa-random"></i></span>ActAIM2 has an interaction mode selector and a low-level action predictor.</li>
            <li><span class="fa-li"><i class="fas fa-cogs"></i></span>The selector generates discrete modes, and the predictor outputs corresponding actions.</li>
            <li><span class="fa-li"><i class="fas fa-chart-line"></i></span>Our experiments show ActAIM2 improves robotic manipulation and generalization over baselines.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Blog Reading Session -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Unlocking Robotic Intelligence: How ActAIM2 Is Changing the Game for Interaction Modes</h2>
        <div class="content has-text-justified">
          <p>
            Imagine a robot that intuitively knows whether to open or close a drawer, selecting the appropriate action without any prior instruction or explicit programming. This level of autonomy has long been a challenge in robotics. However, recent advancements in AI and robotics by Liquan Wang and his team are turning this vision into reality with their innovative <strong>ActAIM2</strong> model.
          </p>

          <h3 class="title is-4">What's New in Robotic Learning?</h3>
          <p>
            In traditional robotics, teaching machines to recognize and act on different manipulation modes has been a significant hurdle. Most models struggle without direct supervision or predefined expert labels, limiting their ability to adapt to new tasks or environments. Enter <strong>ActAIM2</strong>—a breakthrough that equips robots with the ability to understand and execute complex tasks by learning interaction modes from scratch, without external labels or privileged simulator data.
          </p>

          <h3 class="title is-4">Introducing ActAIM2: A New Way to Learn</h3>
          <p>
            ActAIM2 distinguishes itself with a dual-component structure:
          </p>
          <ul>
            <li><strong>Interaction Mode Selector</strong>: A smart module that captures and clusters different interaction types into discrete representations.</li>
            <li><strong>Low-Level Action Predictor</strong>: A companion module that interprets these modes and generates precise actions for the robot to execute.</li>
          </ul>

          <h3 class="title is-4">How Does ActAIM2 Work?</h3>
          <p>
            Think of ActAIM2 as a self-taught explorer. It observes simulated activities and picks up on the nuances of each task, using self-supervised learning to create clusters of interaction types. For example, the model can group actions related to opening or closing an object and then learn the specific movements required for each.
          </p>
          <p>
            Key techniques that power ActAIM2 include:
          </p>
          <ul>
            <li><strong>Generative Modeling</strong>: The mode selector uses generative processes to identify differences between initial and final states.</li>
            <li><strong>Multiview Fusion</strong>: To build a robust understanding, the model integrates observations from multiple angles into a comprehensive visual input.</li>
          </ul>

          <h3 class="title is-4">Why Is This Important?</h3>
          <p>
            This method marks a significant shift in how robots learn to interact with their environments:
          </p>
          <ul>
            <li><strong>No Human Labels Needed</strong>: ActAIM2’s unsupervised learning approach means it doesn't rely on manually labeled data, making it highly adaptable and scalable.</li>
            <li><strong>Improved Manipulability</strong>: By breaking down tasks into discrete interaction modes, robots can handle new tasks more efficiently.</li>
            <li><strong>Enhanced Generalization</strong>: The model’s design enables it to apply what it learns to different scenarios, boosting performance across various tasks.</li>
          </ul>

          <h3 class="title is-4">Real-World Implications</h3>
          <p>
            The potential impact of ActAIM2 spans multiple industries:
          </p>
          <ul>
            <li><strong>Manufacturing</strong>: Robots that can autonomously switch between complex tasks like assembling or disassembling products.</li>
            <li><strong>Healthcare</strong>: Assistive robots capable of safely operating in dynamic environments by understanding nuanced human requests.</li>
            <li><strong>Service and Hospitality</strong>: Robots that can anticipate and perform tasks such as serving food or tidying spaces without specific training for each action.</li>
          </ul>

          <h3 class="title is-4">Final Thoughts</h3>
          <p>
            The development of ActAIM2 represents a significant leap forward in autonomous learning for robots, unlocking the ability for machines to learn, adapt, and perform with minimal human oversight. It’s not just about creating more capable robots; it’s about making them smarter, more efficient, and better integrated into human-centered tasks. This innovation opens the door to a future where machines are not just tools but active, intelligent collaborators in our daily lives.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End of Blog Reading Session -->

<!-- Image carousel -->
<section class="section hero is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Method</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/figures/teaser_3.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          ActAIM2 identifies meaningful interaction modes such as open and close drawers from RGB-D images of articulated objects and robots.
          It represents these modes as discrete clusters of embeddings and trains a policy to generate control actions for each cluster-based interaction.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/figures/Fig2_3.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          GMM Model Selector The mode selector, a generative model, processes the differences between
  the initial and final image visual embeddings as generated data, using the initial image embeddings as the
  conditional variable.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/figures/fig3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Behavior Cloning Action Predictor Interaction mode ε is sampled from latent space
  embedding from model selector. 5 Multiview RGBD observations from circled cameras are back-projected and
  fused into a color point cloud to render novel views. Rendered image tokens and interaction mode token are
  contacted and fed through a multiview transformer to predict action a = (p, R, q).
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/figures/mode_selector.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Mode Selector Decoder Architecture: The depicted architecture highlights the functionality
  of the mode selector decoder, which is designed to process two primary inputs: multi-view RGBD images
  O i = (O 0 i , O 1 i , O 2 i , O 3 i , O 4 i ), and the Mixture of Gaussian (GMM) variable x. It is important to note that x
  can be represented as a multi-view feature vector, with our encoding approach preserving the separation of
  multi-view channels. Initially, the multi-view RGBD images are passed through a pre-trained VGG-19 image
  encoder to extract feature vectors for each view. Subsequently, these feature vectors, along with the GMM
  variable x, are inputted into a joint transformer. This transformer, featuring four attention layers, is tasked with
  producing the means and variances associated with the reconstructed task embedding ϵ.
      </h2>
    </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/figures/action_predictor.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Action Predictor Architecture: This model integrates multi-view observations directly as input,
  sourced from predefined cameras within the scene. The process begins with the extraction of five RGBD images,
  which are subsequently transformed into RGB point clouds. These are then subject to orthogonal projection
  to generate five novel view images. Subsequently, these novel views are partitioned into smaller patches and
  fed into a joint transformer. This transformer, characterized by four attention layers, integrates the sampled
  task embedding derived from a Mixture of Gaussian distribution. The architecture of the joint transformer
  encompasses eight attention layers, culminating in the production of a heatmap. This heatmap delineates the
  action’s translation, the discretized rotation, and a binary variable indicating the gripper’s state—open or closed.
      </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/figures/mode_selector_training.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Training Process of the Mode Selector: This figure illustrates the training procedure of the mode selector,
  mirroring the approach of a conditional generative model. It highlights the contrastive analysis between the
  initial and final observations—the latter serving as the ground truth for task embedding—to delineate generated
  data against the backdrop of encoded initial images as the conditional variable. The process involves inputting
  both the generated task embedding data and the conditional variable into a 4-layer Residual network-based mode
  encoder, which then predicts the categorical variable c. Following the Gaussian Mixture Variational Autoencoder
  (GMVAE) methodology, the Gaussian Mixture Model (GMM) variable x is computed and introduced alongside
  the conditional variable to the task embedding transformer decoder. This model is tasked with predicting the
  reconstructed task embedding, sampled from the Gaussian distribution as outlined in the architecture of the
  mode selector decoder, and calculating the reconstruction loss against the input ground truth data.
      </h2>
    </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/figures/mode_selector_inference.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Action Predictor Architecture: This model integrates multi-view observations directly as input,
  sourced from predefined cameras within the scene. The process begins with the extraction of five RGBD images,
  which are subsequently transformed into RGB point clouds. These are then subject to orthogonal projection
  to generate five novel view images. Subsequently, these novel views are partitioned into smaller patches and
  fed into a joint transformer. This transformer, characterized by four attention layers, integrates the sampled
  task embedding derived from a Mixture of Gaussian distribution. The architecture of the joint transformer
  encompasses eight attention layers, culminating in the production of a heatmap. This heatmap delineates the
  action’s translation, the discretized rotation, and a binary variable indicating the gripper’s state—open or closed.
      </h2>
     </div>

  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="section hero">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">More Qualitative Results</h2>
      <div id="qual_0" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/figures/qual_0.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        qual_0
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/figures/qual_1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        qual_1
        </h2>
      </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/figures/qual_2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        qual_2
        </h2>
      </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/figures/qual_3.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        qual_3
        </h2>
      </div>
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/figures/qual_4.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        qual_4
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Single Video Section -->
<section class="section hero is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Demonstrations on Real World</h2>
      <div id="video-demo" class="video-container">
        <video poster="" id="video1" autoplay controls muted loop height="100%">
          <!-- Your video file here -->
          <source src="static/videos/_CORL_2024__real_world.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>
<!-- End Single Video Section -->

<!-- Video carousel -->
<section class="section hero">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Demonstrations on Simulator</h2>
      <div id="Video Results" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_154_0.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_154_1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_154_2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video4" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_8961_0.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video5" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_8961_1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video6" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_8961_2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video7" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_19898_0.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video8" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_19898_1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video9" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_19898_2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video10" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_20555_0.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video11" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_20555_1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video12" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_41083_0.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video13" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_41083_1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video14" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_41083_2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video15" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_102844_0.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video16" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/video_102844_1.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<!--BibTeX citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{wang2024discoveringroboticinteractionmodes,
      title={Discovering Robotic Interaction Modes with Discrete Representation Learning}, 
      author={Liquan Wang and Ankit Goyal and Haoping Xu and Animesh Garg},
      year={2024},
      eprint={2410.20258},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2410.20258}, 
}</code></pre>
    </div>
</section>
<!--End BibTeX citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
